variational â†’ we deal with function sets!

----

## Latent Variable Models

main idea: #lvm
>describe high-dimensional data $x$ using few latent factors $z$ for modeling $p_\theta(x)$

![[LVMs.svg]]

two-step process:
1. generate/sample latent variable $z$: $$z \sim p_\theta(z)$$
2. generate data $x$ conditional on $z$: $$x \sim p_{\theta}(x\mid z)$$
which gives us the joint distribution $$p_{\theta(x,}z) = p_{\theta(z)}p_{\theta}(x\mid z)$$and marginal likelihood $$p_{\theta}(x) = \int p_{\theta}(x,z) dz = \int p_{\theta}(z)p_{\theta}(x\mid z) dz = \mathbb{E}_{z\sim p_{\theta}(z)}[p_{\theta}(x\mid z)]$$

#### Example: Gaussian Mixture Models
latent variables: clusters
TODO


### Tasks in LVMs

1. **inference**: extract latent features #lvminference $$p_{\theta(z}\mid x) = \frac{p_{\theta}(x \mid z)p_{\theta}(z)}{p_{\theta}(x)}$$
2. **learning**: find parameters $\theta$ #lvmlearning $$\max_{\theta} \log p_{\theta}(X) = \max_{\theta} \frac{1}{N} \sum\limits_{i=1}^{N} \log p_{\theta}(x_{i})$$
### MLE in LVMs

for single sample $x$: $$\begin{align*}
\max_{\theta} \log p_{\theta}(x) &= \max_{\theta} \log \left(\int p_{\theta}(x, z) dz \right) \\
&= \max_{\theta} \log \left( \int p_{\theta}(x \mid z) p_{\theta}(z) dz \right) \\
&= \max_{\theta} f(\theta)
\end{align*}$$BUT the integral doesn't have a closed-form solution!

*NOTE: with NFs, we could use reverse parametrization to easily obtain our MLE*

----

## Maximization using Lower Bounds
â†’ we learn $\theta$ via inference/by finding the posterior distribution ðŸ«¥

- instead of maximizing intractable $f(\theta)$ (with intractable $\nabla f$), maximize lower bound $g(\theta)$
- pick a set of functions $\mathcal{G}$ for a better result $$\max_{\theta} f(\theta) \geq \max_{g\in\mathcal{G}} \max_{\theta} g(\theta)$$aka find best lower bound

#### Lower Bound for Marginal Log-likelihood
let $q(z)$ be an arbitrary distribution over $z$: $$
\log p_{\theta} (x) = \underbrace{\mathbb{E}_{z\sim q(z)} \left[ \frac{p_{\theta}(x, z)}{q(z)} \right]}_{\mathcal{L}(\theta, q)} + \underbrace{\mathbb{KL} (q(z) \mid\mid p_{\theta}(z \mid x))}_{\geq 0}$$where 
- $\mathbb{KL}$ is the **Kullback-Leibler Divergence** $$\mathbb{KL}(q(z) \mid \mid p(z)) := \int q(z) \log\frac{q(z)}{p(z)} dz$$which is a similarity measure between distributions
	- asymmetric
	- non-negative
	- 0 iff $q=p$ almost everywhere
- $\mathcal{L}(\theta, q)$ is an **E**vidence **L**ower **Bo**und (ELBO)
	- lower bound on evidence $\log p_{\theta}(x)$
	- which we now need to maximize

â†’ we can rewrite the ELBO as follows: $$\mathcal{L}(\theta, q) = -\mathbb{KL}(q(z) \mid\mid p_{\theta}(z \mid x)) + \log p_{\theta}(x)$$and as $\mathbb{KL}=0$ for $p=q$, we can (in an ideal setting) pick $q(z) = p_{\theta}(z \mid x)$ and arrive at $$\mathcal{L}(\theta, q) = \log p_{\theta}(x)$$
so:
>for any fixed $\theta$: maximizing ELBO w.r.t $q$ $\Leftrightarrow$ make $q(z)$ as close as to $p_{\theta}(z \mid x)$ as possible

##### Computing $p_{\theta}(z \mid x)$
- often also intractable
- use #EM 

##### EM and Variational Inference

E-step: picking another function $$\begin{align*}
q^{(t)}(z) &= p_{\theta^{(t)}} (z \mid x)\\
&= \arg \min_{q} \mathbb{KL}(q(z) \mid\mid p_{\theta^{(t)}}(z \mid x))\\
&= \arg \max_{q} \mathcal{L}(\theta^{(t)}, q)
\end{align*}$$M-step: maximizing new function $$\begin{align*}
\theta^{(t+1)} &= \arg\max_{\theta} \mathbb{E}_{z \sim q^{(t)}(z)} [\log p_{\theta}(x, z)]\\
&= \arg\max_{\theta} \mathcal{L}(\theta, q^{(t)})
\end{align*}$$

![8egXBCs](8egXBCs.gif)

-----

## Optimizing the ELBO
$$\max_{\theta, q} \mathcal{L(\theta, q)}$$*PROBLEM*: how to optimize w.r.t $q(z)$, which is a probability distribution?

*SOLUTION*: **parametric family of distributions**
e.g.: 1D exponential distributions, all distributions that can be modelled via NF, etc
- pick a set of candidate tractable parametric distributions $\mathcal{Q}$: $$\max_{\theta \in \mathbb{R}^{M}, q \in \mathcal{Q}} \mathbb{E}_{z \sim q(z)} [\log p_{\theta}(x, z) - \log q(z)]$$with $\mathcal{Q} = \{ q_{\phi}(z) \text{ for } \phi\in\mathbb{R}^{K}\}$
- $\phi$ is a parameter **vector** â†’ we know how to optimize over those!$$\max_{\theta \in \mathbb{R}^{M}, \phi\in\mathbb{R}^K} \mathbb{E}_{z \sim q_{\phi}(z)} [\log p_{\theta}(x, z) - \log q_{\phi}(z)]$$
