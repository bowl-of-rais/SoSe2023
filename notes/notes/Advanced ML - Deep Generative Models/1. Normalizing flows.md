----

#### Motivation

assumption: data $x$ follows probability distribution $p(x)$, aka $$p(x) \text{ where } x = \begin{pmatrix}x_1 \\ \vdots \\ x_D\end{pmatrix}$$
then we want to
- **generate sample data** following the distribution
- **evaluate densities** given a data point

it is often easier to model a complex distribution by applying transformations to simpler distributions, but then we need a way to transform the input variables! hence: **change of variables**

----

## Change of Variables Formula

### Examples

##### Idea behind NF
$D=1, p_1(z) = Unif([0,1]), f(z) = 2z + 1 = x$

![[Pasted image 20230420095356.png]]

to have valid probability densities, both should integrate to 1:

$$p_1(z)\delta z = p_2(x) \delta x$$
which then leads us to $$p_2(x) = p_1(z)\frac{\partial z}{\partial x} = p_1(z)\frac{\partial g(x)}{\partial x} = p_1(z) \cdot 0.5$$
where $$g(x) = f^{-1}(x) = \frac{x-1}{2} = z$$
the 'direction' does not matter, hence we can just use the absolute value
- magnitude of the gradient $$p_2(x) = p_1(z)\left|\frac{\partial z}{\partial x}\right|$$
- factor renormalizes probability distribution

##### What happens for shifts?
constant shifts do not change the area after the transformation

##### What happens for linear transformation?
$D=1, p_1(z) = Unif([0,1]^2), f(z) = Mz = x, M = \begin{pmatrix}a & b \\ c & d\end{pmatrix}$
- area is changed from $1$ to $ad-bc = \det(M)$
- normalization: $$p_2(x) = p_1(z)\left|\frac{1}{\det(M)}\right|$$
- fun fact: $\frac{1}{\det(M)} = \det(M^{-1})$

### Change of Variables: General formula

> for $D \in \mathbb{N}$, $p_1(z)$ a $D$-dimensional distribution, $f(z)=x$ an *invertible* and *differentiable* transformation, then
> $$p_2(x) = p_1(f^{-1}(x)) \cdot \left|\det\left(\frac{\partial f^{-1}(x)}{\partial x}\right)\right|$$ #changeofvariables

- determinant term: **distortion rate** of transformation
	- transformation is locally linear, i.e. if we 'zoom in' enough, it looks like it's a linear transformation ([relevant stackexchange post](https://math.stackexchange.com/questions/3496035/how-are-we-able-to-locally-linearize-a-non-linear-transform-with-the-jacobian-ma))/it can be locally approximated with a linear function
- $\frac{\partial g(x)}{\partial x}$ is a **Jacobian** of $g$
	- remember: $x \in \mathbb{R}^D$ is a vector → the Jacobian is  $D \times D$ matrix
	- also: $$\frac{\partial f^{-1}(x)}{\partial x} = \left(\frac{\partial f(z)}{\partial z}\right)^{-1}$$which means: applying the *inverse function* to the *transformation output* $x$ and then taking the derivative is the same as the *inverse gradient* of applying the *original function* to the *original input* → depends on what is known!

our goal now: find good transformations $f$ where we can calculate the term easily

##### Conditions for a valid transformation

1. **invertibility**
	- input and output space have **same dimension** $D$
	- for $D=1$: $f$ being strictly monotonic is enough
	- for linear transformation $f$: it should be $\det(f)\neq 0$

2. **differentiability**
	- $f$ and $f^{-1}$ are continuously differentiable
		- this means: Jacobian $\frac{\partial f^{-1}(x)}{\partial x}$ exists at any point
	- *sufficient* (hinreichende) condition

### Stacking

multiple variables: $z_{i-1}$ is transformed by $z_i = f_i(z_{i-1})$
- last transformation goes from $z_{k-1}$ to $z_k = x$
- NF only produces normalized distributions, so the final distribution is also normalized
- we need to combine the normalization factors by multiplying

change of variables formula for stacked transformations: $$
p_K(x) = p_0(z_0) \prod^K_{i=1} \left|\det\left(\frac{\partial f_i^{-1}(z_i)}{\partial z_i}\right)\right|$$"Normalizing Flow": input variable flows through multiple transformations

stacked NFs are similar to NNs, but:
- NN layers are in general not invertible
- in particular, they often change the dimensionality

### Logarithmic version

for $D \in \mathbb{N}$, $D$-dimensional distribution $p_1(z)$, inverdiffable transformation $f(z)=x$:$$\log(p_2(x)) = \log(p_1(f^{-1}(x))) + \log\left| \det\left(\frac{\partial f^{-1}(x)}{\partial x}\right) \right|$$ and when stacking transformations: $$
\log(p_K(x)) = \log(p_0(z_0)) + \sum^K_{i=1} \log\left|\det\left(\frac{\partial f_i^{-1}(z_i)}{\partial z_i}\right)\right|$$
----

## Forward and Reverse Parametrization

1. **reverse parametrization**: evaluate $p_2(x)$ at any point x
	- specified: $g(x)$
2. **forward parametrization**: sample points from $p_2(x)$
	- specified: $f(g)$

parametrization differs between types of flows (planar/radial, [realNVP](https://paperswithcode.com/method/realnvp), IAF, MAF, spline) 
- some flows are more efficient for evaluating, some for sampling
- both (at the same time) might not be required in an application

### 1. Reverse Parametrization

APPROACH: parametrize (known!) inverse transformation $g = f^{-1}$: $$g_{\varphi}(x)=z$$where $\varphi$ can be learned

$f=g^{-1}$ exists as well, but we might not know it analytically
→ $g_{\varphi}^{-1}(z) = x$ might not be easily computable

CoV formula with reverse parametrization:$$p_2(x) = p_1(g_\varphi(x)) \cdot \left|\det\left(\frac{\partial g_{\varphi}(x)}{\partial x}\right)\right|$$which only uses $g_\varphi$, so we can compute $p_2$ at any point $x^{(j)}$! 

##### Reverse Parametrization and Stacking
for stacked transformations with reverse parametrization$$g_\varphi=g_{\varphi_1}\circ \cdots \circ g_{\varphi_K}$$we can compute the density as follows:
1. given $x^{(j)}$, set $x^{(j)} = z_K$
2. compute $z^{(j)}_{i-1} = g_{\varphi_i}(z_i^{(j)})$ and $\left|\det\left(\frac{\partial g_{\varphi}(z_i^{(j)})}{\delta z_i^{(j)}}\right)\right|$ for each $i$
	- chain of $z_K$s
	- we need to 'go in reverse' to gather all the normalization terms
1. given $z_0^{(j)}$ compute $p_0(z_0^{(j)})$ and thus $p_K(x^{(j)})$

##### Why reverse parametrization?
→ we can use the computed density for learning

### 2. Forward parametrization

APPROACH: parametrize (known!) transformation $f$:$$f_\theta(z)=x$$where $\theta$ can be learned

$f^{-1}$ exists as well, but we might not know it analytically
→ $f_{\theta}^{-1}(x) = z$ might not be easily computable

CoV formula with forward parametrization:$$p_2(x) = p_1(z) \cdot \left|\det\left(\frac{\partial f_{\theta}(z)}{\partial z}\right)\right|^{-1}$$only uses $f_\theta$, so we can compute a sample $x^{(j)} \sim p_2(x)$ and $p_2(x^{(j)})$ from another sample $z^{(j)}$

##### Forward Parametrization and Stacking
for stacked transformations with reverse parametrization$$f_\theta=f_{\theta_K}\circ \cdots \circ f_{\theta_1}$$we can sample from $p_K(x)$ as follows:
1. sample $z_0^{(j)}\sim p_0(z_0)$ 
	  → easy distribution like Uniform or Gaussian
2. compute $z^{(j)}_{i} = f_{\theta_i}(z_{i-1}^{(j)})$ and $\left|\det\left(\frac{\partial f_{\theta_i}(z_{i-1}^{(j)})}{\partial z_{i-1}^{(j)}}\right)\right|^{-1}$ for each $i$
3. for a **particular sample** $x^{(j)}=z_K^{(j)}$ we can compute $p_K(x^{(j)})$ 

##### Why reverse parametrization?
→ [Variational Inference](2.1.%20Variational%20inference.md): sample $x$ from distribution $q$ and compute probability $q(x)$ for that sample


-----

## Jacobian Determinant Computation

Jacobian computation can be slow (blowup!):$$x=\begin{bmatrix}x_1 \\ \vdots \\ x_D\end{bmatrix} \qquad g(x)=\begin{bmatrix}g_1(x) \\ \vdots \\ g_D(x)\end{bmatrix} \qquad J_g = \begin{bmatrix}\frac{\partial g_1(x)}{\partial x_1} & \cdots & \frac{\partial g_1(x)}{\partial x_D} \\ \vdots & \ddots & \vdots \\ \frac{\partial g_D(x)}{\partial x_1} & \cdots & \frac{\partial g_D(x)}{\partial x_D}\end{bmatrix}$$→ we need ways to effectively compute it!

1. Diagonal Jacobian
2. Triangular Jacobian
3. Full Jacobian

##### Refresher: Determinant properties
determinant of inverse:$$\det(A^{-1}) = \frac{1}{\det(A)}$$determinant and eigenvalues:$$\det(A)=\prod^D_{i=1}\lambda_i$$determinant and block matrices:$$\det(A)=\det(B)\det(C) \qquad\text{for}\qquad A=\begin{bmatrix}B & 0 \\ 0 & C\end{bmatrix}$$

### Diagonal Jacobian
If the function is applied element-wise$$g(x) = \begin{bmatrix}g_\textcolor{turquoise}1(x_{\textcolor{turquoise}1}) \\ \vdots \\g_\textcolor{turquoise}D(x_{\textcolor{turquoise}D})\end{bmatrix}$$then the Jacobian will be diagonal matrix because $\frac{\partial g_i(x)}{\partial x_j} = 0$ for $i \neq j$ and thus, the determinant is the product of the diagonal elements:$$\det(J_g) = \prod^D_{i=1}\frac{\partial g_i(x_i)}{\partial x_i}$$
### Triangular Jacobian
If the function is applied like$$g(x) = \begin{bmatrix}g_\textcolor{turquoise}1(x_{\textcolor{turquoise}1}) \\ \vdots \\g_\textcolor{turquoise}D(x_{\textcolor{turquoise}1}, \dots, x_{\textcolor{turquoise}D})\end{bmatrix}$$then the Jacobian will be a triangular matrix because $\frac{\partial g_i(x)}{\partial x_j} = 0$ for $i > j$ and thus, the determinant is again the product of the diagonal elements:$$\det(J_g) = \prod^D_{i=1}\frac{\partial g_i(x_i)}{\partial x_i}$$which is e.g. used in autoregressive flows

### Full Jacobian
The function is applied in its most general form$$g(x) = \begin{bmatrix}g_\textcolor{turquoise}1(x) \\ \vdots \\g_\textcolor{turquoise}D(x)\end{bmatrix}$$and we hence have the full Jacobian matrix. The determinant can be computed using the LU decomposition:
- $J_g=LU$ where $L$ and $U$ are the lower and upper triangular matrices
- then:$$\det{J_g} = \det(L)\det(U)$$where $\det(L)$ and $\det{U}$ are diagonal products. This takes $\mathcal{O}(D^3)$ time!
