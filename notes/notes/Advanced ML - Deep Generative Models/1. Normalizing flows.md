----

#### Motivation

assumption: data $x$ follows probability distribution $p(x)$, aka $$p(x) \text{ where } x = \begin{pmatrix}x_1 \\ \vdots \\ x_D\end{pmatrix}$$
then we want to
- **generate sample data** following the distribution
- **evaluate densities** given a data point

it is often easier to model a complex distribution by applying transformations to simpler distributions, but then we need a way to transform the input variables! hence: **change of variables**

----

## Change of Variables Formula

#### Examples

##### Idea behind NF
$D=1, p_1(z) = Unif([0,1]), f(z) = 2z + 1 = x$

![[Pasted image 20230420095356.png]]

to have valid probability densities, both should integrate to 1:

$$p_1(z)\delta z = p_2(x) \delta x$$
which then leads us to $$p_2(x) = p_1(z)\frac{\partial z}{\partial x} = p_1(z)\frac{\partial g(x)}{\partial x} = p_1(z) \cdot 0.5$$
where $$g(x) = f^{-1}(x) = \frac{x-1}{2} = z$$
the 'direction' does not matter, hence we can just use the absolute value
- magnitude of the gradient $$p_2(x) = p_1(z)\left|\frac{\partial z}{\partial x}\right|$$
- factor renormalizes probability distribution

##### What happens for shifts?
constant shifts do not change the area after the transformation

##### What happens for linear transformation?
$D=1, p_1(z) = Unif([0,1]^2), f(z) = Mz = x, M = \begin{pmatrix}a & b \\ c & d\end{pmatrix}$
- area is changed from $1$ to $ad-bc = \det(M)$
- normalization: $$p_2(x) = p_1(z)\left|\frac{1}{\det(M)}\right|$$
- fun fact: $\frac{1}{\det(M)} = \det(M^{-1})$

### Change of Variables: General formula

> for $D \in \mathbb{N}$, $p_1(z)$ a $D$-dimensional distribution, $f(z)=x$ an *invertible* and *differentiable* transformation, then
> $$p_2(x) = p_1(f^{-1}(x)) \cdot \left|\det\left(\frac{\partial f^{-1}(x)}{\delta x}\right)\right|$$

- determinant term: **distortion rate** of transformation
	- transformation is locally linear, i.e. if we 'zoom in' enough, it looks like it's a linear transformation ([relevant stckexchange post](https://math.stackexchange.com/questions/3496035/how-are-we-able-to-locally-linearize-a-non-linear-transform-with-the-jacobian-ma))
- $\frac{\partial g(x)}{\partial x}$ is a **Jacobian** of $g$
	- remember: $x \in \mathbb{R}^D$ is a vector â†’ the Jacobian is  $D \times D$ matrix
	- also: $$\frac{\partial f^{-1}(x)}{\partial x} = \left(\frac{\partial f(z)}{\partial z}\right)^{-1}$$
our goal now: find good transformations $f$ where we can calculate the term easily

##### Conditions for a valid transformation

1. **invertibility**
	- input and output space have same dimension $D$
	- for $D=1$: $f$ being strictly monotonic is enough
	- for linear transformation $f$: it should be $\det(f)\neq 0$

2. **differentiability**
	- $f$ and $f^{-1}$ are continuously differentiable
		- this means: Jacobian $\frac{\partial f^{-1}(x)}{\partial x}$ exists at any point
	- *sufficient* (hinreichende) condition

### Stacking

multiple variables: $z_{i-1}$ is transformed by $z_i = f_i(z_{i-1})$
- last transformation goes from $z_{k-1}$ to $z_k = x$

change of variables formula for stacked transformations: $$
p_K(x) = p_0(z_0) \prod^K_{i=1} \left|\det\left(\frac{\partial f_i^{-1}(z_i)}{\partial z_i}\right)\right|$$"Normalizing Flow": input variable flows through multiple transformations

### Logarithmic version

for $D \in \mathbb{N}$, $D$-dimensional distribution $p_1(z)$, inverdiffable transformation $f(z)=x$:$$\log(p_2(x)) = \log(p_1(f^{-1}(x))) + \log\left| \det\left(\frac{\partial f^{-1}(x)}{\partial x}\right) \right|$$ and when stacking transformations: $$
\log(p_K(x)) = \log(p_0(z_0)) + \sum^K_{i=1} \log\left|\det\left(\frac{\partial f_i^{-1}(z_i)}{\partial z_i}\right)\right|$$
----

## Forward and Reverse Parametrization

[TODO]

## Jacobian Determinant Computation

[TODO]