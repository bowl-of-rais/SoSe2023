-----

so far: linear score functions $$f_{i} = \sum\limits_{i} w_{i,j}x_{j} \text{ or } f = Wx$$cannot be improved upon by simply adding matrices, bc the result is still only a linear transformation

----

## Neural Network

example: 2-layer network $f = W_{2} \max(0, W_{1}x)$
![[2-layer network.svg]]
or:
![[NN-layer-structure.svg]]

*NOTE: during this process, the dimensionality is changed from the input dimension (e.g. 128x128 pixels) to the output dimension (e.g. 10 classes)*

#### Artificial Neurons

calculation in single neuron: $$f(W_{layer,\> neuron}x + b_{layer,\> neuron})$$where $x\in \mathbb{R}^{n}$ is a multi-dimensional input vector
- typically, the $W$ in each layer have the same dimensionality
- $b$ is some scalar added onto the dot product

calculation in whole layer: $$f(W_{layer}x + b_{layer})$$with matrix-vector multiplication
- $b$ is now a vector
- helps with abstraction and doing fast multiplications on GPUs

#### Activation Functions
| name            | function |
| --------------- | -------- |
| sigmoid         | $\sigma(x) = \frac{1}{1+e^{-x}}$         |
| tanh            |  $\tanh(x)$        |
| ReLU            | $\max(0, x)$         |
| leaky ReLU      |  $\max(0.1x, x)$        |
| parametric ReLU | $\max(\alpha x, x)$         |
| maxout          | $\max(w_{1}^{T} + b_{1}, w_{2}^{T}x + b_{2})$         |
| ELU             |  $f(x)=\begin{cases} x & x > 0 \\ \alpha(e^x - 1) & x \leq 0\end{cases}$        |

#### How to find weights and biases?

- stochastic gradient descent to minimize loss function
- gradients are calculated using backpropagation
- iterate many times over training set #sgd

---

## Computational Graphs

- directed graph
- vertices: variables and operators (kinda like syntax tree)
- edges: show flow of inputs
- great for representing neural networks in a more abstract way

##### Example: Forward pass
$f(x,y,z) = (x + y) \cdot z$
![[computational-graph.svg]]

>**forward pass**: feed values into input and compute along graph

##### Computations in Neural Networks
- $Wx$ encodes input info
- activation function selects key features
- convolutional layers: extract features with shared weights

----

## Loss functions
→ how to describe how close network's outputs are to targets

>**loss function**: measures goodness of predictions → large loss means bad predictions

##### Regression Loss
- L1 loss: $$L(y, \hat{y}; \theta) = \frac{1}{n} \sum\limits_{i}^{n} ||y_{i}- \hat{y}_{i}||_{1}$$
- MSE loss: $$L(y, \hat{y}; \theta) = \frac{1}{n} \sum\limits_{i}^{n} ||y_{i}- \hat{y}_{i}||_{2}^{2}$$
##### Binary Cross Entropy
loss function for binary classification $$L(y, \hat{y}; \theta) = -\frac{1}{n} \sum\limits_{i}^{n} [y_{i}\log \hat{y}_{i} + (1-y_{i})\log(1-\hat{y}_{i})]$$
##### General Cross Entropy
loss function for multi-class classification $$L(y, \hat{y}; \theta) = -\sum\limits_{i}^{n} \sum\limits_{i}^{k} (y_{ik}\log \hat{y}_{ik})$$
### Improving Neural Networks
→ optimization problem: minimize loss w.r.t. $\theta$

- in NNs: gradient-based optimization$$\theta= \theta-\alpha\nabla_{\theta}L(y, f_{\theta}(x))$$for some **learning rate**/step size $\alpha$

##### Example: one-layer NN with MSE loss
inputs $x$, targets $y$, loss $L(y, \hat{y}; \theta) = \frac{1}{n} \sum\limits_{i}^{n} ||y_{i}- \hat{y}_{i}||_{2}^{2}$
compute graph: 
![[computegraph_mse.svg]]
gradient flows 'backwards' through the graph: $$\nabla_{\theta}L(y, f_{\theta}(x)) = \frac{2}{n} \sum\limits_{i}^{n} (W \cdot x_{i} - y_{i}) \cdot x^{T}_{i}$$
##### Gradients in Multi-layer NNs
- **backpropagation**: use chain rule → go through every node

##### Why Gradient Descent?
- easy to compute using compute graphs
- other methods: Newtons method, L-BFGS, adaptive moments, conjugate gradient