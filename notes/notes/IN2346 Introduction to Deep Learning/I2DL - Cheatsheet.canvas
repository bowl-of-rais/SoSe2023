{
	"nodes":[
		{"type":"group","id":"df1e87855d536f79","x":-680,"y":-200,"width":1125,"height":1260,"label":"Basics: Machine Learning"},
		{"type":"group","id":"4a7b970c26cae2fd","x":640,"y":80,"width":940,"height":1024,"label":"Basics: Neural Networks"},
		{"id":"2f5b456d1b406bd9","x":-642,"y":380,"width":1049,"height":620,"type":"group","label":"Loss functions"},
		{"type":"text","text":"#kNN : compare representations using distance function, pick majority class among $k$ nearest neighbors","id":"519d468e2ee19a3c","x":-640,"y":-158,"width":325,"height":120},
		{"type":"text","text":"**linear regression**:$$\\hat{y}_{i} = \\theta_{0} + \\sum\\limits_{j=1}^{d}x_{ij}\\theta_{j}$$or in matrix notation $\\hat{y}=X \\theta$","id":"ecd0f02c66210658","x":-617,"y":84,"width":280,"height":160,"color":"6"},
		{"id":"81674987fd511dcd","x":-298,"y":229,"width":255,"height":103,"type":"text","text":"**classification**: predict a discrete value\n→ binary or multi-class"},
		{"type":"text","text":"**regression**: predict continuous output value","id":"41bdcb90d26ea237","x":-252,"y":57,"width":200,"height":107},
		{"type":"text","text":"hyperparameters:\n- distance function\n- number of neighbors $k$","id":"6c2496c7a58e248f","x":-234,"y":-108,"width":260,"height":120},
		{"id":"62e926564bcee656","x":-623,"y":690,"width":325,"height":220,"color":"6","type":"text","text":"**linear least squares**: #lss $$\\min_{\\theta}J(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}(\\hat{y}_{i} - y_{i})^{2}$$or $$\\min_{\\theta} J(\\theta) = (X \\theta - y)^{T}(X \\theta - y)$$"},
		{"id":"f2f51436fff94d1a","x":-445,"y":438,"width":422,"height":140,"type":"text","text":"$$p(y_{i} \\mid x_{i}, \\theta) =-\\frac{n}{2} \\log(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} (y-X\\theta)^{T}(y-X \\theta)$$assuming $y_{i} \\sim \\mathcal{\\mu, \\sigma^{2}}$"},
		{"id":"9a76fa5854a45772","x":30,"y":400,"width":360,"height":108,"type":"text","text":"**binary cross-entropy**: #bce $$\\mathcal{L}(\\hat{y}_{i}, y_{i}) = - [y_{i} \\log \\hat{y}_{i} + (1 - y_{i})\\log(1 - \\hat{y}_{i})]$$"},
		{"id":"0986e7ccaa7775cf","x":-152,"y":690,"width":500,"height":280,"color":"6","type":"text","text":"**Maximum-Likelihood Estimate**:\nassumes i.i.d.!$$p_{model}(y \\mid X, \\theta) = \\prod_{i=1}^{n} p_{model}(y_{i} \\mid x_{i}, \\theta)$$or in log-space: $$\\theta_{ML} = \\arg \\max_{\\theta} \\sum\\limits_{i=1}^{n} \\log p_{model}(y_{i}\\mid x_{i}, \\theta)$$"},
		{"type":"text","text":"**logistic regression**: $$\\hat{y} = \\prod_{i=1}^{n} \\textcolor{Periwinkle}{\\hat{y}_{i}}^{y_{i}} (1-\\hat{y}_{i})^{(1-\\textcolor{Orchid}{y_{i}})}$$\n-> use sigmoid $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$to convert into probabilities","id":"ef4e58e17550e65f","x":27,"y":57,"width":365,"height":275,"color":"6"},
		{"type":"file","file":"notes/IN2346 Introduction to Deep Learning/data-split.svg","id":"c25ba55d4983376c","x":53,"y":-158,"width":373,"height":140},
		{"type":"text","text":"linear score functions $f = Wx$ cannot be improved by adding more matrices, we need **non-linearity**","id":"48c6d5d5f38bb33b","x":525,"y":200,"width":235,"height":167},
		{"type":"text","text":"NNs can be visualized with a computational graph\n- vertices $\\sim$ variables, operators\n- edges $\\sim$ flow of inputs","id":"53f34d262986eb06","x":710,"y":480,"width":320,"height":160},
		{"type":"text","text":"gradient-based optimization$$\\theta= \\theta-\\alpha\\nabla_{\\theta}L(y, f_{\\theta}(x))$$for some **learning rate**/step size $\\alpha$","id":"89dd9ffbed702f8b","x":710,"y":780,"width":320,"height":140},
		{"type":"text","text":"**backpropagation**: basically using the chain rule","id":"793914bb4ae2a7e4","x":990,"y":890,"width":400,"height":60},
		{"type":"text","text":"**loss function**: measures goodness of predictions\n→ improving NN means minimizing its loss","id":"e5402b50e490c99e","x":680,"y":980,"width":380,"height":104,"color":"6"},
		{"type":"text","text":"**neural network**: nesting of functions","id":"eff27f0a0fd9b2cc","x":830,"y":100,"width":321,"height":60},
		{"type":"text","text":"calculation per neuron: $$f(W_{layer,\\> neuron}x + b_{layer,\\> neuron})$$calculation per layer: $$f(W_{layer}x + b_{layer})$$","id":"59f75e0f53f59ad1","x":852,"y":200,"width":276,"height":163,"color":"6"},
		{"type":"text","text":"activation functions $f$ introduce non-linearity","id":"2737e1c2ba41ebe0","x":1089,"y":333,"width":220,"height":67},
		{"type":"text","text":"intuition behind computation\n- $Wx$ encodes input info\n- activation function selects key features","id":"fbcb92f2b74c679e","x":1230,"y":137,"width":308,"height":145},
		{"type":"text","text":"| name            | function |\n| --------------- | -------- |\n| sigmoid         | $\\sigma(x) = \\frac{1}{1+e^{-x}}$         |\n| tanh            |  $\\tanh(x)$        |\n| ReLU            | $\\max(0, x)$         |\n| leaky ReLU      |  $\\max(0.1x, x)$        |\n| parametric ReLU | $\\max(\\alpha x, x)$         |\n| maxout          | $\\max(w_{1}^{T} + b_{1}, w_{2}^{T}x + b_{2})$         |\n| ELU             |  $f(x)=\\begin{cases} x & x > 0 \\\\ \\alpha(e^x - 1) & x \\leq 0\\end{cases}$        |","id":"932a84c7d7471bc9","x":1060,"y":480,"width":369,"height":366}
	],
	"edges":[
		{"id":"28e813ade1a5f0bd","fromNode":"519d468e2ee19a3c","fromSide":"right","toNode":"6c2496c7a58e248f","toSide":"left","toEnd":"none"},
		{"id":"93d54c202fc20567","fromNode":"41bdcb90d26ea237","fromSide":"left","toNode":"ecd0f02c66210658","toSide":"right"},
		{"id":"830d9a946e4e3027","fromNode":"41bdcb90d26ea237","fromSide":"right","toNode":"ef4e58e17550e65f","toSide":"left"},
		{"id":"1f5abd0f5d8e00f2","fromNode":"df1e87855d536f79","fromSide":"right","toNode":"48c6d5d5f38bb33b","toSide":"left","toEnd":"none"},
		{"id":"66dc2502fe9a1e46","fromNode":"eff27f0a0fd9b2cc","fromSide":"bottom","toNode":"59f75e0f53f59ad1","toSide":"top","toEnd":"none"},
		{"id":"52634847941ee096","fromNode":"2737e1c2ba41ebe0","fromSide":"bottom","toNode":"932a84c7d7471bc9","toSide":"top","toEnd":"none"},
		{"id":"c2e66de1f2fc962a","fromNode":"59f75e0f53f59ad1","fromSide":"right","toNode":"fbcb92f2b74c679e","toSide":"left","toEnd":"none"},
		{"id":"489d09a933514e20","fromNode":"59f75e0f53f59ad1","fromSide":"bottom","toNode":"53f34d262986eb06","toSide":"top","toEnd":"none"},
		{"id":"a757140690d14d8f","fromNode":"89dd9ffbed702f8b","fromSide":"top","toNode":"53f34d262986eb06","toSide":"bottom","label":"gradients can be easily \ncomputed using graph"},
		{"id":"a3ea18be9dce29bd","fromNode":"89dd9ffbed702f8b","fromSide":"bottom","toNode":"e5402b50e490c99e","toSide":"top","toEnd":"none"},
		{"id":"2fb3db25518b6b82","fromNode":"ef4e58e17550e65f","fromSide":"bottom","toNode":"9a76fa5854a45772","toSide":"top","toEnd":"none"},
		{"id":"b5f9bdb0baf48177","fromNode":"ecd0f02c66210658","fromSide":"bottom","toNode":"62e926564bcee656","toSide":"top","toEnd":"none"},
		{"id":"99ff59cc28fd95a6","fromNode":"ecd0f02c66210658","fromSide":"bottom","toNode":"f2f51436fff94d1a","toSide":"top","toEnd":"none"},
		{"id":"8af8bd01aa6ac644","fromNode":"9a76fa5854a45772","fromSide":"bottom","toNode":"0986e7ccaa7775cf","toSide":"top","toEnd":"none"},
		{"id":"f18caea8c5ef1469","fromNode":"81674987fd511dcd","fromSide":"right","toNode":"ef4e58e17550e65f","toSide":"left"},
		{"id":"226a60f3e3a19692","fromNode":"f2f51436fff94d1a","fromSide":"bottom","toNode":"0986e7ccaa7775cf","toSide":"top"},
		{"id":"f951e1c47f558694","fromNode":"e5402b50e490c99e","fromSide":"left","toNode":"2f5b456d1b406bd9","toSide":"right","toEnd":"none"}
	]
}