-----

## Motivation

**sequence of observations** $X_1, \dots, X_T$
- index $t$: time, location, ...
- for now: cont observation at discrete time steps
- example: time-series dorecasting in weather, economics,...

> observations are not independent → non-i.i.d data


## Definition

> $AR(p)$ of order $p$:
> $$ X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \epsilon_t$$ where $\varphi_i$ are parameters, $\epsilon\sim N(0, \sigma)$ is **white noise** and $X_{t-i}$ is the **lagged value** at time $i$
> #autoregression

order $p$ means we consider the $p$ previous time steps!
 
- linear regression
- **auto**regressive: previous outputs/lags as inputs
- every time point affects all future time points → no independence
- $\varphi$ and $c$ are **shared through time**

rewrite as probabilistic model:
$$P(X_t \mid X_{t-1}, \dots, X_{t-p}) \sim N(c+\sum^p_{i=1} \varphi_i X_{t-i}, \sigma)$$

##### Mean function of an AR model
$$\mu(t) = E[X_t]$$

##### Auto[covariance](https://en.wikipedia.org/wiki/Covariance)
$$\gamma(t, i) = Cov(X_t, X_{t-1})$$

##### Pearson autocorrelation function
$$\rho(t,i) = \frac{Cov(X_t, X_{t-i})}{\sqrt{Var(X_t)}\sqrt{Var(X_{t-i})}} \in [-1,1]$$
→ normalized covariance, shows e.g. seasonalities

#### Variance and autoregression
- usually, "x-distance" is relevant for whether variance is low or high
	- is the known data far in the past?
- with autoregression, "y-distance" is more important
	- have we seen similar data before?


## Parameter Learning #learnAR

least squares regression:
$$\begin{bmatrix}\varphi_1 \\ \vdots \\ \varphi_p\end{bmatrix}=(X^TX)^{-1}X^Ty$$
where
$$X=\begin{bmatrix}X_{p-1} & \cdots & X_0 \\ X_p & \cdots & X_1 \\\vdots & \cdots & \vdots\end{bmatrix}$$
and $$y = \begin{bmatrix}X_p \\ X_{p+1} \\ \vdots\end{bmatrix}$$


## Data split

**must** be according to time
→ test only on future values

-----
## Stationarity

> a **stationary** process...
> 1. has a **constant mean** function: $E[X_t] = E[X_{t-i}] \forall t, i$
> 2. has an autocovariance that depends only on the lagged value, not $t$: $Cov(X_t, X_{t-i}) = \gamma_i \forall t, i$, this also means: $\gamma_i = Cov(X_t, X_{t-i}) = Cov(X_{t-i}, X_t) = \gamma_{-i}$
> 3. $E[|X_t|^2] < \infty\>\forall t$
> #stationaryAR

- good modeling assumption (like i.i.d)
- estimate mean/autocovariance by estimating over time: generalize to future!

#### Using Stationarity to learn parameters:

#Yule-Walker equations: express covariance value of steps with distance $i$ using covariances of other distances$$\begin{align*}\gamma_{0}&= \sum\limits^{p}_{j=1}\varphi_{j}\gamma_{-j}+\sigma^{2} \\ \gamma_{1}&= \sum\limits^{p}_{j=1}\varphi_{j}\gamma_{1-j} \\
\dots \\
\gamma_{p}&= \sum\limits^{p}_{j=1}\varphi_{j}\gamma_{p-j}
\end{align*}$$or in matrix form:$$\begin{bmatrix}\gamma_{1}\\ \gamma_{2}\\ \vdots \\ \gamma_{p-1}\\ \gamma_{p}\end{bmatrix} = \begin{bmatrix} \gamma_{0} & \gamma_{-1} & \cdots  & \gamma_{2-p} & \gamma_{1-p}\\ \gamma_{1} & & & & \gamma_{2-p} \\ \vdots & & \ddots & & \vdots \\ \gamma_{p-2}& & & & \gamma_{-1} \\ \gamma_{p-1} & \gamma_{p-2}& \cdots  & \gamma_{1}& \gamma_0\end{bmatrix}\begin{bmatrix}\varphi_{1}\\ \varphi_2\\ \vdots\\ \varphi_{p-1} \\ \varphi_p\end{bmatrix}$$
##### Parameters can now be learned as follows:
1. estimate moments $\gamma_{0}, \dots, \gamma_p$
2. inverse Yule-Walker matrix to estimate $\varphi_{1}, \dots, \varphi_p$
3. use equation for $\gamma_{0}$ to estimate $\sigma$

*WHY? - because the $\gamma_i$ can be estimated more reliably, therefore are preferably used as input instead of the raw data*