----

> **Markov chain**: sequence of random variables $X_1, X_2, \dots, X_T$ with the **Markov property**: $$P(X_t | X_{1}, \dots, X_{t-1}) = P(X_t | X_{t-1})$$
> where $t$ and $X_i$ are discrete
> → joint distribution:$$P(X_1=i_{1}, \dots, X_T=i_{T)}= P(X_{1}= i_{1})\prod^{T-1}_{t=1} P(X_{t+1} = i_{t+1} \mid X_{t}= i_t)$$ #markovchain

in general, the distribution of each random variable can be different - we therefore define
- a **prior probability** vector $\pi\in\mathbb{R}^K$ with $\pi_{i}= P(X_{1}=i)$
- **transition matrices** $A^{(t)}\in\mathbb{R}^{K\times K}$ with $A_{ij}^{(t+1)}=P(X_{t+1}=j\mid X_{t}=i)$

joint probability in matrix notation $$P(X_1=i_{1,}\dots, X_T=i_{T})= \pi_{i1}\times A^{(2)}_{i1,i2}\times\cdots\times A^{(T)}_{i_{T-1},i_T}$$which has $(K-1)+(T-1)K(K-1)$ parameters

graphical model:
![MC-general](MC-general.png)

----

## Stationary/Time-homogenous Markov Chains

→ single transition matrix independent of $t$

the joint probability becomes $$P(X_1=i_{1,}\dots, X_T=i_{T)}= \pi_{i1}\times A_{i1,i2}\times\cdots\times A_{i_{T-1},i_T}$$which has $(K-1)+K(K-1)$ parameters

![stationary MC](MC-stat.png)

----

## Markov Chains as a Random Walk

- time-homogenous discrete MCs can be interpreted as state machines

##### Example: weather conditions
state machine:
![[ex-weather.svg]]
corresponding matrix:$$A = \begin{bmatrix}\textcolor{ProcessBlue}{0.6} & \textcolor{OrangeRed}{0.2} & \textcolor{FUchsia}{0.2} \\ \textcolor{ProcessBlue}{0.1} & \textcolor{OrangeRed}{0.5} & \textcolor{FUchsia}{0.4} \\ \textcolor{ProcessBlue}{0.4} & \textcolor{OrangeRed}{0.1} & \textcolor{FUchsia}{0.5}\end{bmatrix}$$where each column represents the probability of ending up in the specific state
→ sequence: random walk, aka multiply the probabilties with eachother

also possible for higher-order Markov chains, but that would exponentially create new states

----

## Learning parameters of a MC

given set $X_{1:T_n}^{(n)}$ of $N$ observed sequences, learn $\pi$ and $A$ using maximum-likelihood

the probability of observing the sequences is a product of the probabilities associated with each sequence:$$P(all) = \prod^{N}_{n=1} P(X_1^{(n)})\cdot\prod^{T_n-1}_{t=1} \Pr(X_{T+1}^{(n)} \mid X_t^{(n)})$$which we can re-write to refer to the total number of states $K$: $$=\left( \prod^{K_{k=1}}\pi_k^{L(k)} \right)\left( \prod^{K}_{i=1}\prod^{K}_{j=1} A_{ij}^{N(i,j)} \right)$$where
- $L(k)$ denotes the number of sequences starting in state $k$
- $N(i,j)$ denotes the number of transitions from state $i$ to state $j$ across all sequences

we can then apply the logarithm:$$\log(P(all))=\log \sum\limits^{K}_{k=1}L(k)\log(\pi_k) + \sum\limits^{K}_{i=1}\sum\limits^{K}_{j=1} N(i,j)\log(A_{ij})$$
we then need to maximize this term and subject it to
- $\sum_{k}\pi_k=1$ 
- $\sum\limits_{j}A_{ij}=1$
to arrive at$$A_{ij}=\frac{N(i,j)}{\sum\limits_{j'} N(i, j')} \qquad \pi_k=\frac{L(k)}{\sum\limits_{k'}L(k')}$$
→ basically: use relative frequency

----

## Other Fun Stuff to do with MCs

##### Probability of getting from $i$ to $j$ in $n$ steps
recursive function: $$A_{ij}(n) = P(X_{t=n}=j\mid X_t = i) = \sum^K_{k=1} A_{kj}A_{ik}(n-1)$$or in matrix form: $$A(n) = A(n-1)A$$which in combination with $A(1)=A$ gives us $$A(n)=A^n$$and we arrive at the **Chapman-Kolmogorov** equations: $$A_{ij}(m+n)=\sum^K_{k=1} A_{ik}(m)A_{kj}(n)$$or (Potenzregel!)$$A(m+n)=A(m)+A(n)$$
>TL;DR in stationary Markov chains, multi-step probabilities are just exponentiations of $A$

##### Probability of reaching state $j$ in step $t$
recursive function: $$\pi_j(t)=\Pr(X_t=j) = \sum^{K}_{i=1} \Pr(X_{t=j}\mid X_{t-1}=i)\Pr(X_{t-1}=i) = \sum\limits^{K}_{i=1}A_{ij}\pi_{i}(t-1)$$which gives us$$\pi(t)=\pi(t-1)A$$or$$\pi(t)=\pi A^{(t-1)}$$aka start probability * $t-1$ transitions
where $\pi(t)$ and $\pi$ are row vectors