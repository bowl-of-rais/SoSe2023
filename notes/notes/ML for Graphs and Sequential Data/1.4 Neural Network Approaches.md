----

#### Motivation
- representing words to input into a subsequent model
- words as vectors → keep underlying language properties
	- in particular: similarity

>distributional hypothesis: words that appear in similar contexts have similar meanings

----

## Co-occurrence

- incorporate context by counting how many times a word appeared besides other words
- slide window and count co-occurrences → matrix $M$
- similar words have similar vectors, but matrix is highdimensional and sparse → dimensionality reduction

#### [SVD](https://de.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)
$$M = U \Sigma V^T$$where the first $D$ columns of $U$ are $D$-dimensional word vectors
→ slow to compute, hard to add new words

-----

## Word2vec

TASK: prediction of words based on context → **self-supervised learning**

1. **continuous bag-of-words**: predicts word from window
	- weak for rare words
2. **skip-gram**: predict context from current words
	- rare words must be understood, so good for those
	- slower to train but good for small datasets + rare words

### Skip-gram

- input: one-hot vector $\in\mathbb{R}^N$
- embedding: project onto $D$-dimensions using $U \in \mathbb{R}^{N\times D}$
- prediction: obtain probabilities of context words with $X \in \mathbb{R}^{D\times N}$

which combines into $$\begin{align*}
\max_{\theta}\mathbb{E}[P(S \mid x_{i}, \theta)] &= \min_\theta(-\mathbb{E}[P(S \mid x_{i}, \theta)])\\
P(S \mid x_{i}, \theta) &= \prod_{x_{k}\in S} P(x_{k} \mid x_{i}, \theta)\\
P(x_{k} \mid x_{i}, \theta) &= \text{softmax}(u_{i}V)_{k}
\end{align*}$$
#### Training
- softmax normalization is very costly

alternative: **Negative Sampling**
- each iteration: sample a positive word $p$ in the context of word $i$ and negative word(s) $n$ not in the context
- train classification


-----

## RNNs

reccurent neural network building block:
![[RNN block.svg]]
- problem types
	- one to many
	- many to one
	- many to many

### Definition

start with inputs $\{x^{(1)}, \dots, x^{(N)}\}$

hidden state $h^{(t-1)}$ represents sequence $\{x^{(1)}, \dots, x^{(t-1)}\}$ and is used to predict output $y^{(t)}$

![[RNN-unrolled.svg]]

### Objective


##### Backpropagation through time
- $h^{(t)}$ depends on all previous time steps
- gradient $\frac{\partial L}{\partial h^{(t)}}$ depends on future steps
	- more specifically, depends on product of future steps
	- impact of future times may vanish or explode
	- RNN cannot retain info for many steps

NOTE: this just means it cannot be trained, in theory basic RNNs can perform well


### GRU

add **gates** that control information flow$$\begin{align*}z^{(t)} &= \sigma(W_{z}[h^{(t-1)}, x^{(t)}]) \\
r^{(t)} &= \sigma(W_{r}[h^{(t-1)}, x^{(t)}]) \\
\tilde{h}^{(t)} &= \tanh (W[r^{(t)} \odot h^{(t-1)}, x^{(t)}])\\
h^{(t)} &= (1-z^{(t)}) \odot h^{(t-1)} + z^{(t)}\odot\tilde{h}^{(t)}
\end{align*}$$

### LSTM
more intricate gates:
- forget gate
- input gate
- output gate $$\begin{align*}
f^{(t)} &= \sigma(W_{f}[h^{(t-1)}, x^{(t)}])\\
i^{(t)} &= \sigma(W_{i}[h^{(t-1)}, x^{(t)}])\\
o^{(t)} &= \sigma(W_{o}[h^{(t-1)}, x^{(t)}])\\
c^{(t)} &= f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot \tanh(W[h^{(t-1)}, x^{(t)}])\\
h^{(t)} &= o^{(t)} \odot \tanh(c^{(t)})
\end{align*}$$and new cell state $c^{(t)}$

----

## Non-recurrent models

introduction

- RNNs depend on full history
- alternative: convolutional neural networks

#### Recap: convolution
$$(f \star g)(t) = \int_{-\infty}^{+\infty} f(\tau)g(t-\tau) d\tau$$


### WaveNet

problem in 1D: we should only look into the past, not the future
- [causal convolutions](https://paperswithcode.com/method/causal-convolution)
- [dilated convolutions](https://paperswithcode.com/method/dilated-causal-convolution)

difference to #autoregression : 
- joint training means full data still affects learned weights
- but during application time


### Transformers
parallelizable with attention

structure: madeup of encoder and decoder stacks

##### Attention
- convolution: weighting in window of fixed size
- attention: weighting of whole input, but weights are different for each output

self-attention weighting mechanism:
![[attention.svg]]
→ self-attention: same input for all 3 (query, key, value)

in matrix notation: $$\text{softmax}\left(\frac{Q \times K^{T}}{\sqrt{d_{k}}}\right)V = Z$$

##### Encoder Block
- word embeddings are fed into attention layer, results are fed into FFNN
- attention block is shared among inputs/embeddings, the rest is handled independently

##### Positional Encoding
- in general, attention mechanisms do not care about token order
- workaround: positional encoding added/concatenated with embedding
	- basically: each position gets its own vector
	- the same word can be represented differently based on embedding

##### Training
- focus attention on history (mask attention for future tokens)
- aka only use full input and previous outputs
	- keys and values only depend on past
- outputs: probabilities over all possible words → sample output word from that
- while application can run in parallel, training still needs to be done in sequential manner

##### Complexity
- quadratic complexity → one weight per input and output
- intractable for long sequences

possible solutions:
- different structure of attention weights
- low-rank approximations
- downsampling sequence length n

### GPT Models
**G**enerative **P**re-trained **T**ransformer

training:
1. unsupervised pre-training on predicting next token in a sequence
2. task-specific fine-tuning
→ large text corpora, very large models

transfer learning:
- either copy parts (usually lower layers)
- or use pre-trained parts as warm start (with frozen layers)

foundation models:
- train single basic model on large dataset
	- learn okay-ish first layers
- then use transfer learning to focus on specific task