----

#### Overview: HMM Tasks
| Problem   | Algorithm                                                                                                              | Time Complexity |
| --------- | ---------------------------------------------------------------------------------------------------------------------- | --------------- |
| Filtering | [Filtering: Forwards Algorithm](1.3%20Hidden%20Markov%20Models.md#Filtering:%20Forwards%20Algorithm)                   |  $\mathcal{O}(TK^{2})$               |
| Smoothing | [Smoothing: Forward-Backwards Algorithm](1.3%20Hidden%20Markov%20Models.md#Smoothing:%20Forward-Backwards%20Algorithm) |           $\mathcal{O}(TK^{2})$      |
| MAP       | [MAP inference in HMMs](1.3%20Hidden%20Markov%20Models.md#MAP%20inference%20in%20HMMs)                                 |          $\mathcal{O}(TK^{2})$       |
| Learning  | [Parameter Learning in HMMs](1.3%20Hidden%20Markov%20Models.md#Parameter%20Learning%20in%20HMMs)  - Variational Inference or Baum-Welch                     |             $\mathcal{O}(TK^{2})$    |

#### When to use HMMs or sth else?
depends on
- latent space
- observation space

|                              | no latent space                                           | discrete latent space | continuous latent space |
| ---------------------------- | --------------------------------------------------------- | --------------------- | ----------------------- |
| discrete observation space   | [Markov Chains](1.2%20Markov%20Chains.md)                 | discrete HMM          | no default              |
| continuous observation space | [Autoregressive Models](1.1%20Autoregressive%20Models.md) | continuous HMM        | e.g. Kalman filter      | 


-----

#### Motivation

- basic AR models and MC are VERY simple - too simple
- now: probabilistic latent variable models for sequences of observations

1. Markov property is not realistic
	- eg: NLP often has long-range dependencies
2. states are often not known and can only be observed indirectly
	- measurements may be noisy and a subset of the 'true' sequence

>while the true, unobserved states may fully describe the situation to the point of having the Markov property, the indirect measurements often do not

----

## Definition

>**Hidden Markov Model**: sequence of **hidden variables** $[Z_1,\dots,Z_T]$ and sequence of **observed variables** $[X_1,\dots,X_T]$ s.t. the $Z_1, \dots, Z_T$ satisfy the **Markov property**: $$P(Z_{t+1} \mid Z_t, Z_{t-1}... TODO)$$and the distribution of $X_t$ depends only on $Z_t$: $$P(X_{t+1}\mid Z_{1},\dots,Z_{t}, X_{1,}\dots, X_T)=P(X_{t+1}\mid Z_{t+1})$$
>$P$ is called a **transition probability** and $P$ is called an **emission probability**
> #HMM

convention: discrete time $t$ and discrete $Z_t$

graphical model:
![HMM-general](HMM-general.png)

joint distribution:$$\begin{align*} &P(Z_{1}=z_{1},\dots,Z_{T}=z_{T},X_{1}=x_{1},\dots,X_{T}=x_{T})\\ =\quad &P(Z_1=z_{1};\pi) \prod^{T-1}_{t=1} P(Z_{t+1}=z_{t+1}\mid Z_t=z_{t};\theta^{(t+1)}) \prod^T_{t=1}P(X_{t}=x_{t} \mid Z_{t}=z_{t}; \varphi^{(t)})
\end{align*}$$
### Discrete Case: $X_t \in \{1, 2, \dots, K'\}$

probabilities can be described as matrices - one matrix per time step: #discreteHMM $$\textcolor{SkyBlue}{\begin{align*}P(Z_{1}=i)&=\pi_{i}\\P(Z_{t+1}=j\mid Z_{t}=i) &= A_{ij}^{(t+1)} \\P(X_{t+1}=j\mid Z_{t+1}=i) &= B_{ij}^{(t+1)}\end{align*}}$$but number of parameters is still very high at $K+(T-1)K^{2}+ TKK'$
(one summand per equation)

![HMM-discrete](HMM-discrete.png)

### Parameter Tying

shared parameters → same matrices $A$ and $B$ for all time steps
- now only $K+ K^{2}+ KK'$ parameters
- new joint distribution:$$\begin{align*} &P(Z_{1}=z_{1},\dots,Z_{T}=z_{T},X_{1}=x_{1},\dots,X_{T}=x_{T})\\ =\quad &P(Z_{1}=z_{1};\pi) \prod^{T-1}_{t=1} A_{z_{t}z_{t=1}}\prod^{T}_{t=1}B_{z_{t}x_{t}}
\end{align*}$$

----

## Tasks

1. **inference**: from fixed model parameters, find info from posterior distribution $\Pr(Z-{1:T} \mid X_{1:T})$
2. **parameter learning**: learn model parameters from observed $X_{1:T}$

### Inference

1. **filtering**: compute current time step given sequence up to that point - online setting!
2. **smoothing**: compute a time step given the whole sentence
3. **MAP inference**: compute most likely sequence of hidden states
	- also known as Viterbi decoding
	- might be different from individual filtering!
	- only gives sequence, not the distribution/probability

> the most likely sequence != sequence of most likely states 

e.g. probability of 'that' vs probability of 'that that'

#### Filtering: Forwards Algorithm
> goal: compute $P(Z_t \mid X_{1:t})$ #forwards-algo

Bayes' Rule gives us$$P(Z_t \mid X_{1:t})=\frac{P(Z_t=k \mid X_{1:t})}{\sum\limits_{j=1}^{K} P(Z_{t}= j \mid X_{1:t})}$$and we introduce a new name for the joint probability$$\alpha_{t}(k) \stackrel{def}{=} P(Z_{t}=k, X_{1:t})$$ where$$\alpha_{t}=\begin{bmatrix}\alpha_{t}(1)\\ \vdots \\ \alpha_{t}(K)\end{bmatrix}$$
so $\alpha_{t}$ records for a specific time step $t$ and each state $i$ the joint probability that we are in state $i$ while having observed the sequence from time step $1$ to $t$

hence, we can rewrite our conditional probability for the belief state as$$P(Z_t \mid X_{1:t}) = \frac{\alpha_{t}(k)}{sum(\alpha_{t})}$$
→ from there, we compute $\alpha_1$ :$$\textcolor{SkyBlue}{\alpha_1(k)=\pi_k B_{kx_{1}}}$$ and then $\alpha_{t+1}$ recursively from $\alpha_t$:$$\alpha_{t+1}(k) = B_{k(x_{t+1})}\sum^K_{j=1} A_{jk}\alpha_t(j)$$or in matrix notation:$$\textcolor{SkyBlue}{\alpha_{t+1} = B_{:(x_{t+1})} \odot (A^T\alpha_t)}$$which takes $\mathcal{O}(TK^2)$ time

>after calculation, probabilities should be normalized

NOTES: 
- $\odot$ denotes the elementwise or *Hadamard* product
- the vector taken from $B$ is an emissions *column*, which holds the probabilities of ending up in the specific state 

#### Smoothing: Forward-Backwards Algorithm
> goal: compute $P(Z_t \mid X_{1:T})$ from past *and future* states #fw-bw-algo

Bayes' Rule gives us $$P(Z_{t}=k \mid X_{1:T}) = \frac{P(Z_{t}=k, X_{1:t})P(X_{t+1:T}\mid Z_{t}=k))}{\sum\limits^{K}_{j=1} P(Z_{t}=j, X_{1:T})}$$and we introduce a new name for the conditional probability$$\textcolor{SkyBlue}{\beta_{t}(k) \stackrel{def}{=} P(X_{t+1:T} \mid Z_{t}=k)}$$and $$\beta_{t}=\begin{bmatrix}\beta_{t}(1)\\ \vdots \\ \beta_{t}(K)\end{bmatrix}$$so $\beta_{t}$ records for a specific time step $t$ and for each state $1\leq i \leq K$ the probability of the (known!) sequence of future observations under the assumption that the current state is $i$

we can combine this together with the earlier definition of $\alpha_{t}$ to obtain$$P(Z_{t}=k \mid X_{1:T}) \propto \alpha_{t}(k)\beta_{t}(k)$$
→ from there, we initialize $\beta_T$: $$\beta_T(k)=1$$and then compute $\beta_t$ from $\beta_{t+1}$:$$\textcolor{SkyBlue}{\beta_{t}(j) = \sum\limits_{k=1}^{K} A_{jk} B_{kx_{t+1}} \beta_{t+1}(k)}$$or: transition from $j$ to $k$ * emission from $k$ to observation at $t+1$ * recursion (future state)

and in matrix notation:$$\textcolor{SkyBlue}{\beta_{t}= A(B_{:(x_{t+1})} \odot \beta_{t+1})}$$
##### Applications of the forward-backward algorithm (review!)
1. probability of being in state $k$ at time $t$ online$$P(Z_{t}=k \mid X_{1:t}) = \frac{\alpha_{t}(k)}{\sum\limits_{s} \alpha_{t}(s)}$$
2. probabilty of being in state $k$ at time $t$ offline$$\gamma_{t}(k) := P(Z_{t} = k \mid X_{1:T}) = \frac{\alpha_{t}(k) \beta_{t}(k)}{\sum\limits_{s} \alpha_{t}(s)\beta_{t}(s)}$$
3. probability of two adjacent states having specific realizations $$\xi_{t}(i,j) := P(Z_{t}=i, Z_{t+1}=j \mid X_{1:T}) = \frac{\alpha_{t}(i) A_{ij} \beta_{t+1}(j) B_{jx_{t+1}}}{\sum\limits_{u}\sum\limits_{v} a_{t}(u) A_{uv} \beta_{t+1}(v) B_{vx_{t+1}}}$$
#### MAP inference in HMMs
> goal: given observed sequence $X_1:T$  find most probable sequence of hidden states $z_{1}, \dots, z_{T}$ or  $$\begin{align*}&\arg \max_Z P(Z_{1:T}\mid X_{1:T})\\ = &\arg \max_Z \log P(Z_{1:T}\mid X_{1:T})\\ = &\arg\max_{Z}\log (P(Z_{1})P(X_{1}\mid Z_{1})) + \sum\limits_{t=2}^{T}\log(P(Z_{t}\mid Z_{t-1})P(X_{t}\mid Z_{t}))\end{align*}$$where each term depends on values of $Z_{t-1}$ and $Z_t$ #MAPinHMM

- model: bipartite graph with weights between time steps

>using negative log-likelihood, the problem becomes a minimization problem and when thinking of the graph representation, a **shortest-path-problem**

1. weights of edges from start node: $\textcolor{SkyBlue}{-\log (P(Z_{1}=j) P(X_{1}\mid Z_{1}=j))}$
	- state prob * emission prob
2. weights of intermediate layers: $\textcolor{SkyBlue}{-\log(P(Z_{t}=j \mid Z_{t-1} = i)P(X_{t} \mid Z_{t}=j))}$
	- transition prob * emission prob
3. weights of edges to end node: $\textcolor{SkyBlue}{0}$

→ VITERBI algorithm

alternatively: **longest**-path-problem where (non-logarithmized) probabilities are multiplied along paths
- end edges must be 1

----

## Parameter Learning in HMMs

> goal: learn model parameters $$\theta = \{\pi, A, B\}$$by solving $$\max_{\theta}\log P(X\mid \theta) = \max_{\theta}\log \sum\limits_{Z}P(X \mid Z, \theta)\cdot P(Z\mid \theta)$$

unfortunately, there is no analytical solution (marginalization/latent variables: equation system unsolvable)

iterative solution: #EM algorithm or as named for HMMs: #Baum-Welch-Algorithm

##### Recap: [EM algorithm](https://de.wikipedia.org/wiki/EM-Algorithmus)
- E step: evaluate posterior $P(Z\mid X, \theta^{old})$
- M step: maximize expected joint log-likelihood 
	- $\theta^{new} = \arg\max_{\theta} \mathbb{E}_{P(Z\mid X, \theta^{old})}\log P(X, Z \mid \theta)$ 
	- maximize evidence lower bound aka lower bound on $\log P(X\mid \theta)$
	- more: [DGM - Variational inference](2.1.%20Variational%20inference.md)

### E step for HMMs
posterior: $$P(Z_{1:T} \mid X_{1:T},\theta^{old}) = \frac{P(X_{1:T}\mid Z_{1:T}, \theta^{old})P(Z_{1:T} \mid \theta^{old})}{\sum_{Z_{1:T}} P(X_{1:T}\mid Z_{1:T}, \theta^{old})P(Z_{1:T} \mid \theta^{old})}$$where $$P(X_{1:T}\mid Z_{1:T}, \theta^{old})P(Z_{1:T} \mid \theta^{old}) = \prod_{t=1}^{T} P(X_{1}\mid Z_{t}, \theta^{old})P(Z_{t}\mid Z_{t-1}, \theta^{old})$$so for each $Z_{t}$, we have a **pair** of terms (instead of simple terms like in GMMs)
→ [TODO: check recording - sth with Bayes' law]

### M step for HMMs
joint log-likelihood: $$\begin{align*}
\mathbb{E}_{P(Z_{1:T}\mid X_{1:T}, \theta^{old})} [\log P(X_{1:T}, Z_{1:T} \mid \theta)] &= \sum\limits_{k} \textcolor{Periwinkle}{P(Z_{1}=k \mid X_{1:T}, \theta^{old})} \log(\pi_{k})\\
&+ \sum\limits_{i,j} \sum\limits_{t} \textcolor{Periwinkle}{P(Z_{t}=i, Z_{t+1}=j \mid X_{1:T}, \theta^{old})} \log(A_{ij}) \\
&+ \sum\limits_{i} \sum\limits_{t} \textcolor{Periwinkle}{P(Z_{t}=i \mid X_{1:T}, \theta^{old})} \mathbb{I}(x_{t}=j)\log(B_{ij})
\end{align*}$$where the marked terms can be calculated with the Forward-Backward algorithm

maximize this for $\theta$
- projected gradient ascent
- closed-form bc its available

----

## HMMs for Continuous Data

earlier: discrete $t$, $Z_{t}$ and $X_{t}$
now: discrete $t$ and $Z_{t}$, but continuous $X_{t}$  → measurements are often continuous!
$$\begin{align*}
P(Z_{1}=i) &= \pi_{i}\\
P(Z_{t+1}=j \mid Z_{t}=i) &= A_{ij}\\
P(X_{t+1}=x \mid Z_{t+1}=i) &= \mathcal{N}(x \mid \mu_{i}, \sigma^{2}_{i} \cdot I)
\end{align*}$$

example: $X_{t}$ are 2D Gaussians, $Z_{t}$ can take 3 states
- probability of staying in same state is high
- useful for e.g. **time-series segmentation**

##### Inference for continuous data
largely stays the same, just uses normal distribution instead of categorical: $$P(X_{t}=x \mid Z_{t}=k) = \mathcal{N}(x \mid \mu_{k}, \sigma^{2}_{k} \cdot I)$$
##### Parameter learning for continous data
parameters $\mu_{i}, \sigma_{i}$ instead of $B_{ij}$
- joint log-likelihood: $$\begin{align*}
\mathbb{E}_{P(Z_{1:T}\mid X_{1:T}, \theta^{old})} [\log P(X_{1:T}, Z_{1:T} \mid \theta)] &= \sum\limits_{k} \textcolor{Periwinkle}{P(Z_{1}=k \mid X_{1:T}, \theta^{old})} \log(\pi_{k})\\
&+ \sum\limits_{i,j} \sum\limits_{t} \textcolor{Periwinkle}{P(Z_{t}=i, Z_{t+1}=j \mid X_{1:T}, \theta^{old})} \log(A_{ij}) \\
&+ \sum\limits_{i} \sum\limits_{t} \textcolor{Periwinkle}{P(Z_{t}=i \mid X_{1:T}, \theta^{old})} \textcolor{Rhodamine}{\log(\mathcal{N}(X_{t}\mid \mu_{i}, \sigma-i ^{2}\cdot I))}
\end{align*}$$which can again be easily maximized for $\theta$ using gradients or the closed form
- observation: estimate for $\mu_{i}, \sigma_{i}^{2}$ equivalent to setting in [GMM](https://brilliant.org/wiki/gaussian-mixture-model/)

----

## Example: Continuous Latent Space

- $Z_{t}$ physical vector quantities e.g. position/velocity/etc at $t$
- $X_{t}$ observed noisy measurements at $t$

probabilities: $$\begin{align*}
P(Z_{t+1} \mid Z_{t}) &= \mathcal{N}(Z_{t+1} \mid \textcolor{SkyBlue}{f}(Z_{t}), \sigma_{z}^{2})\\
P(X_{t} \mid Z_{t}) &= \mathcal{N}(X_{t} \mid \textcolor{SkyBlue}{g}(Z_{t}), \sigma_{x}^{2})
\end{align*}$$

##### Neural HMMs / Deep State Space Models
- $f$ and $g$ can be neural networks $f_\psi$ and $g_{\phi}$ with parameters $\psi$ and $\phi$
- then, parameters can be optimized via gradient ascent in M step of #em$$\theta=\{\psi, \phi\}$$
------

## Continuous Time