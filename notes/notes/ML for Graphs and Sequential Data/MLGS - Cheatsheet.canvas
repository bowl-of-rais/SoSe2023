{
	"nodes":[
		{"id":"6a60d984b0a48447","x":1220,"y":-272,"width":860,"height":852,"type":"group","label":"Recurrent Neural Networks"},
		{"id":"37fecb25b899210c","x":629,"y":-420,"width":471,"height":880,"type":"group","label":"Word vectors"},
		{"type":"group","id":"65236e70c1c308a8","x":-479,"y":480,"width":759,"height":470,"label":"Markov Chains"},
		{"type":"group","id":"3a062f37b624a349","x":-450,"y":-120,"width":685,"height":410,"label":"Autoregressive Models"},
		{"id":"4cf5138a3cc28d3d","x":2200,"y":-420,"width":338,"height":400,"type":"group","label":"CNNs for sequential data"},
		{"type":"text","text":"none\n","id":"5811933a729a8b19","x":-161,"y":360,"width":106,"height":60},
		{"type":"text","text":"learning parameters: relative frequencies\n$$A_{ij}=\\frac{N(i,j)}{\\sum\\limits_{j'} N(i, j')} \\qquad \\pi_k=\\frac{L(k)}{\\sum\\limits_{k'}L(k')}$$","id":"f14ef620b53ca28e","x":-459,"y":500,"width":300,"height":130},
		{"type":"text","text":"calculations:\n- probability of getting from $i$ to $j$ in $n$ steps: exponentiation of transition matrix\n- probability of reaching state $j$ in step $t$: $\\pi(t)=\\pi A^{(t-1)}$","id":"e4af444f55a59cc3","x":-459,"y":750,"width":305,"height":180,"color":"#9eaeff"},
		{"type":"text","text":"$AR(p)$ of order $p$: #autoregression \n$$X_t = c + \\sum_{i=1}^p \\varphi_i X_{t-i} + \\epsilon_t$$","id":"d038fef8a6b57ceb","x":-85,"y":-100,"width":300,"height":130,"color":"#9eb1ff"},
		{"type":"text","text":"learning parameters: least squares regression\n#learnAR","id":"7dfbd007fbb3f1e4","x":-430,"y":-85,"width":250,"height":100},
		{"type":"text","text":"#stationaryAR process:\n- $E[X_t] = E[X_{t-i}] \\forall t, i$\n- $Cov(X_t, X_{t-i}) = \\gamma_i \\forall t, i$\n- $E[|X_t|^2] < \\infty\\>\\forall t$","id":"d2cf8bbd0bb42140","x":-60,"y":110,"width":250,"height":160},
		{"type":"text","text":"#Yule-Walker : easier learning with stationarity","id":"3cfede009f2f4f67","x":-430,"y":160,"width":250,"height":60},
		{"type":"text","text":"latent space","id":"7302300752c5f083","x":400,"y":360,"width":140,"height":60},
		{"type":"text","text":"Markov chain: sequence of random variables with Markov property: $$P(X_t | X_{1}, \\dots, X_{t-1}) = P(X_t | X_{t-1})$$","id":"3e8d206cd39ec3a9","x":-74,"y":510,"width":317,"height":110,"color":"#9eaeff"},
		{"type":"text","text":"single transition matrix $A\\in\\mathbb{R}^{K\\times K}$","id":"01d1695729f06731","x":-53,"y":870,"width":276,"height":60},
		{"type":"text","text":"HMM: sequences of\n- hidden variables $Z_i$ with the Markov property\n- observed variables $X_i$ which depend only on respective $Z_i$","id":"0485621edd0c1281","x":381,"y":540,"width":281,"height":176,"color":"#a3b2ff"},
		{"type":"text","text":"two probability matrices per time step: $$\\begin{align*}P(Z_{1}=i)&=\\pi_{i}\\\\P(Z_{t+1}=j\\mid Z_{t}=i) &= A_{ij}^{(t+1)} \\\\P(X_{t+1}=j\\mid Z_{t+1}=i) &= B_{ij}^{(t+1)}\\end{align*}$$","id":"71a3b79e684006a4","x":396,"y":780,"width":250,"height":170},
		{"type":"text","text":"inference tasks\n1. filtering: current time step given previous sequence\n2. smoothing: any time step given whole sequence\n3. MAP inference: most likely hidden sequence","id":"c694edd3829a3afd","x":775,"y":750,"width":425,"height":150},
		{"id":"cd763d0646c660d8","x":670,"y":250,"width":302,"height":170,"type":"text","text":"**Negative Sampling**\n- for word $i$ sample words inside ($p$) and outside ($n$) of context\n- reduces to binary classification"},
		{"id":"fad39eb6f92878cf","x":1260,"y":-40,"width":316,"height":301,"type":"file","file":"notes/ML for Graphs and Sequential Data/RNN-unrolled.svg"},
		{"id":"3c88d3b905419276","x":1248,"y":-68,"width":170,"height":50,"type":"text","text":"$W, U, V$ shared"},
		{"id":"0f9d8d8d3fa0514a","x":1735,"y":111,"width":265,"height":60,"type":"text","text":"alternative architectures for more control"},
		{"id":"9e0413150aa03415","x":2244,"y":300,"width":250,"height":60,"type":"text","text":"TRANSFORMERS"},
		{"type":"text","text":"defined by\n1. prior probability vector $\\pi\\in\\mathbb{R}^K$\n2. transition matrices $A^{(t)}\\in\\mathbb{R}^{K\\times K}$","id":"263f19cd718ffa56","x":-82,"y":656,"width":333,"height":120},
		{"type":"text","text":"defined by\n- transition probability $P(Z_{t} \\mid Z_{t-1})$\n- emission probability $P(X_t \\mid Z_t)$","id":"7a14862e489bb6a4","x":775,"y":578,"width":329,"height":122},
		{"type":"text","text":"same matrices $A$ and $B$ for all time steps","id":"2513f9bc41d7c7a8","x":353,"y":1020,"width":337,"height":60},
		{"id":"912f76ad3256911d","x":1247,"y":460,"width":342,"height":60,"color":"#b3bfff","type":"text","text":"**RNN cannot retain information for long**"},
		{"id":"3c6b9a53d1c4bcfd","x":1688,"y":220,"width":359,"height":340,"type":"text","text":"**LSTM**:\nadd forget, input, output gates $$\\begin{align*}\nf^{(t)} &= \\sigma(W_{f}[h^{(t-1)}, x^{(t)}])\\\\\ni^{(t)} &= \\sigma(W_{i}[h^{(t-1)}, x^{(t)}])\\\\\no^{(t)} &= \\sigma(W_{o}[h^{(t-1)}, x^{(t)}])\\end{align*}$$and an additional cell state $$c^{(t)} = f^{(t)} \\odot c^{(t-1)} + i^{(t)} \\odot \\tanh(W[h^{(t-1)}, x^{(t)}])$$that is used to update the hidden state $$h^{(t)} = o^{(t)} \\odot \\tanh(c^{(t)})$$"},
		{"id":"a4ef3f73594d8987","x":2189,"y":81,"width":360,"height":89,"type":"text","text":"RNNs are inherently sequential,\nbut we want to compute output in parallel"},
		{"id":"3930b24a23a4bbf7","x":1260,"y":309,"width":316,"height":102,"type":"text","text":"backpropagation: gradient at $h^{(t)}$ depends on product of future time steps and may vanish or explode"},
		{"id":"870e1d143739994a","x":1716,"y":-225,"width":303,"height":291,"type":"text","text":"**gated recurrent unit (GRU)**: \nadd two gates $$\\begin{align*}z^{(t)} &= \\sigma(W_{z}[h^{(t-1)}, x^{(t)}]) \\\\\nr^{(t)} &= \\sigma(W_{r}[h^{(t-1)}, x^{(t)}])\n\\end{align*}$$that control information flow from history and current state $$\\begin{align*}\n\\tilde{h}^{(t)} &= \\tanh (W[r^{(t)} \\odot h^{(t-1)}, x^{(t)}])\\\\\nh^{(t)} &= (1-z^{(t)}) \\odot h^{(t-1)} + z^{(t)}\\odot\\tilde{h}^{(t)}\n\\end{align*}$$"},
		{"id":"954c834241f48562","x":1593,"y":-200,"width":95,"height":194,"type":"file","file":"notes/ML for Graphs and Sequential Data/RNN block.svg"},
		{"id":"e3ff3597cc8e9269","x":1385,"y":-163,"width":250,"height":60,"type":"text","text":"main feature: recursion"},
		{"id":"d39068977d34b7ae","x":879,"y":220,"width":186,"height":60,"type":"text","text":"softmax training = expensive"},
		{"id":"d458732bb9e1f9bc","x":699,"y":-43,"width":340,"height":210,"color":"#a8b7ff","type":"text","text":"**skip-gram**: learn \n- embedding vector $U \\in \\mathbb{R}^{N\\times D}$\n- context-probability vector $V \\in \\mathbb{R}^{D\\times N}$\n$$P(x_{k} \\mid x_{i}, \\theta) = \\text{softmax}(u_{i}V)_{k}$$"},
		{"id":"2a9698328469cc4c","x":684,"y":-226,"width":371,"height":123,"type":"text","text":"**word2vec**: learn word vectors with a NN\n1. **CBOW**: predict word from window\n2. **skip-gram**: predict context from word"},
		{"id":"918ab902e289e1a0","x":711,"y":-380,"width":316,"height":117,"type":"text","text":"goal:\n- represent words as vectors\n- keep underlying properties"},
		{"id":"16bf78d06f01b192","x":2220,"y":-400,"width":294,"height":108,"type":"text","text":"problem vs 2D: we should only use past time steps, not a symmetrical context"},
		{"id":"7b53a027189ba5c3","x":2261,"y":-260,"width":213,"height":60,"type":"text","text":"[causal convolutions](https://paperswithcode.com/method/causal-convolution)"},
		{"id":"cf248057759fee68","x":2300,"y":-100,"width":218,"height":60,"type":"text","text":"[dilated convolutions](https://paperswithcode.com/method/dilated-causal-convolution)"}
	],
	"edges":[
		{"id":"312409b05d041076","fromNode":"d038fef8a6b57ceb","fromSide":"bottom","toNode":"d2cf8bbd0bb42140","toSide":"top","toEnd":"none","label":"stationary"},
		{"id":"55bf739c7a22cdbe","fromNode":"d2cf8bbd0bb42140","fromSide":"left","toNode":"3cfede009f2f4f67","toSide":"right"},
		{"id":"4d4e2b90a1d34f9c","fromNode":"7dfbd007fbb3f1e4","fromSide":"bottom","toNode":"3cfede009f2f4f67","toSide":"top","toEnd":"none"},
		{"id":"ec1ef9f16c53eb8b","fromNode":"d038fef8a6b57ceb","fromSide":"left","toNode":"7dfbd007fbb3f1e4","toSide":"right"},
		{"id":"6736f310ba5e2849","fromNode":"3e8d206cd39ec3a9","fromSide":"bottom","toNode":"263f19cd718ffa56","toSide":"top","toEnd":"none"},
		{"id":"2dcfe22dda2b3b9b","fromNode":"263f19cd718ffa56","fromSide":"bottom","toNode":"01d1695729f06731","toSide":"top","toEnd":"none","label":"stationary"},
		{"id":"f9fc5cddf2fb0981","fromNode":"3e8d206cd39ec3a9","fromSide":"left","toNode":"f14ef620b53ca28e","toSide":"right","toEnd":"none"},
		{"id":"795890d9830b687b","fromNode":"01d1695729f06731","fromSide":"left","toNode":"e4af444f55a59cc3","toSide":"right"},
		{"id":"cabe6c137ac8a1ec","fromNode":"5811933a729a8b19","fromSide":"top","toNode":"3a062f37b624a349","toSide":"bottom","toEnd":"none"},
		{"id":"ca119e0583b74b14","fromNode":"5811933a729a8b19","fromSide":"bottom","toNode":"65236e70c1c308a8","toSide":"top","toEnd":"none"},
		{"id":"69e70159c39d55c1","fromNode":"5811933a729a8b19","fromSide":"right","toNode":"7302300752c5f083","toSide":"left","toEnd":"none"},
		{"id":"7c83c096694e8c58","fromNode":"3e8d206cd39ec3a9","fromSide":"right","toNode":"0485621edd0c1281","toSide":"left","toEnd":"none"},
		{"id":"d8bc805fa1691c16","fromNode":"0485621edd0c1281","fromSide":"right","toNode":"7a14862e489bb6a4","toSide":"left","toEnd":"none"},
		{"id":"fc124bedda51d51b","fromNode":"0485621edd0c1281","fromSide":"bottom","toNode":"71a3b79e684006a4","toSide":"top","toEnd":"none","label":"discrete observations"},
		{"id":"ab362d9cccf6df6d","fromNode":"71a3b79e684006a4","fromSide":"bottom","toNode":"2513f9bc41d7c7a8","toSide":"top","toEnd":"none","label":"parameter tying"},
		{"id":"8f6ca0bcbfe0e82d","fromNode":"0485621edd0c1281","fromSide":"right","toNode":"c694edd3829a3afd","toSide":"left"},
		{"id":"6cb090ddb24263b6","fromNode":"2a9698328469cc4c","fromSide":"bottom","toNode":"d458732bb9e1f9bc","toSide":"top"},
		{"id":"49700fa1a8931f8d","fromNode":"d458732bb9e1f9bc","fromSide":"bottom","toNode":"cd763d0646c660d8","toSide":"top","toEnd":"none","label":"train"},
		{"id":"6df0f05ca00601d2","fromNode":"fad39eb6f92878cf","fromSide":"bottom","toNode":"3930b24a23a4bbf7","toSide":"top","toEnd":"none"},
		{"id":"1d10b503eacd7630","fromNode":"3930b24a23a4bbf7","fromSide":"bottom","toNode":"912f76ad3256911d","toSide":"top"},
		{"id":"6cd4d0c0dcd43983","fromNode":"fad39eb6f92878cf","fromSide":"right","toNode":"0f9d8d8d3fa0514a","toSide":"left","label":"gating"},
		{"id":"76362d69ecd6e1ba","fromNode":"912f76ad3256911d","fromSide":"right","toNode":"0f9d8d8d3fa0514a","toSide":"left"},
		{"id":"6f43de968b9cd5a3","fromNode":"0f9d8d8d3fa0514a","fromSide":"top","toNode":"870e1d143739994a","toSide":"bottom","toEnd":"none"},
		{"id":"61d34bcb4077b660","fromNode":"0f9d8d8d3fa0514a","fromSide":"bottom","toNode":"3c6b9a53d1c4bcfd","toSide":"top","toEnd":"none"},
		{"id":"fcd2b9b083e7494a","fromNode":"7b53a027189ba5c3","fromSide":"bottom","toNode":"cf248057759fee68","toSide":"top","label":"increase receptive field"},
		{"id":"161e8a0ae59ea0db","fromNode":"16bf78d06f01b192","fromSide":"bottom","toNode":"7b53a027189ba5c3","toSide":"top","toEnd":"none"},
		{"id":"7b5cce146959fc74","fromNode":"6a60d984b0a48447","fromSide":"right","toNode":"a4ef3f73594d8987","toSide":"left","toEnd":"none"},
		{"id":"d7088241a6e66aad","fromNode":"a4ef3f73594d8987","fromSide":"top","toNode":"4cf5138a3cc28d3d","toSide":"bottom","toEnd":"none","label":"weigh input window of fixed size"},
		{"id":"f76b8935c5431bd6","fromNode":"a4ef3f73594d8987","fromSide":"bottom","toNode":"9e0413150aa03415","toSide":"top","label":"weigh whole input with output-specific weights"}
	]
}