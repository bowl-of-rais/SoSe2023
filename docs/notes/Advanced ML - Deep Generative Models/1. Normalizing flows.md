----

#### Motivation

assumption: data $x$ follows probability distribution $p(x)$, aka $$p(x) \text{ where } x = \begin{pmatrix}x_1 \\ \vdots \\ x_D\end{pmatrix}$$
then we want to
- **generate sample data** following the distribution
- **evaluate densities** given a data point

[TODO]

----

## Change of Variables Formula

#### Examples

##### Idea behind NF
$D=1, p_1(z) = Unif([0,1]), f(z) = 2z + 1 = x$

![[Pasted image 20230420095356.png]]

to have valid probability densities, both should integrate to 1:

$$p_1(z)\delta z = p_2(x) \delta x$$
which then leads us to $$p_2(x) = p_1(z)\frac{\partial z}{\partial x} = p_1(z)\frac{\partial g(x)}{\partial x} = p_1(z) \cdot 0.5$$
where $$g(x) = f^{-1}(x) = \frac{x-1}{2} = z$$
the 'direction' does not matter, hence we can just use the absolute value
- magnitude of the gradient $$p_2(x) = p_1(z)\left|\frac{\partial z}{\partial x}\right|$$
- factor renormalizes probability distribution

##### What happens for shifts?
constant shifts do not change the area after the transformation

##### What happens for linear transformation?
$D=1, p_1(z) = Unif([0,1]^2), f(z) = Mz = x, M = \begin{pmatrix}a & b \\ c & d\end{pmatrix}$
- area is changed from $1$ to $ad-bc = \det(M)$
- normalization: $$p_2(x) = p_1(z)\left|\frac{1}{\det(M)}\right|$$
- fun fact: $\frac{1}{\det(M)} = \det(M^{-1})$

#### Change of Variables: General formula

> for $D \in \mathbb{N}$, $p_1(z)$ a $D$-dimensional distribution, $f(z)=x$ an *invertible* and *differentiable* transformation, then
> $$p_2(x) = p_1(f^{-1}(x)) \cdot \left|\det\left(\frac{\partial f^{-1}(x)}{\delta x}\right)\right|$$

- determinant term: distortion rate
- 
[TODO]
also true for non-linear functions

our goal now: find good transformations f where we can calculate the term

## Forward and Reverse Parametrization

## Jacobian Determinant Computation